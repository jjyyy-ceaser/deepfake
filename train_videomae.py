import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import cv2
import os
import numpy as np
from tqdm import tqdm
import argparse
from transformers import VideoMAEForVideoClassification

# ==========================================
# âš™ï¸ 1. VideoMAE ì „ìš© ì„¤ì •
# ==========================================
BASE_DIR = "C:/Users/leejy/Desktop/test_experiment/dataset"
SEQUENCE_LENGTH = 16
IMG_SIZE = 224 # VideoMAE ëª¨ë¸ì˜ í‘œì¤€ ì…ë ¥ í•´ìƒë„

# VideoMAE ê³µì‹ ì •ê·œí™” ê°’ ì ìš©
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ==========================================
# ğŸ“‚ 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ==========================================
class VideoSequenceDataset(Dataset):
    def __init__(self, data_dir, sequence_length=16, transform=None):
        self.data_dir = data_dir
        self.seq_len = sequence_length
        self.transform = transform
        self.samples = []
        
        real_dir = os.path.join(data_dir, "real")
        fake_dir = os.path.join(data_dir, "fake")
        
        # mp4 í™•ì¥ì ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê²Œ íƒìƒ‰
        if os.path.exists(real_dir):
            for f in os.listdir(real_dir):
                if f.lower().endswith('.mp4'): self.samples.append((os.path.join(real_dir, f), 0))
        if os.path.exists(fake_dir):
            for f in os.listdir(fake_dir):
                if f.lower().endswith('.mp4'): self.samples.append((os.path.join(fake_dir, f), 1))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        video_path, label = self.samples[idx]
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        start_frame = 0
        if total_frames > self.seq_len:
            start_frame = np.random.randint(0, total_frames - self.seq_len)
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        frames = []
        for _ in range(self.seq_len):
            ret, frame = cap.read()
            if not ret:
                frame = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)
            else:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if self.transform:
                frame = self.transform(frame)
            frames.append(frame)
        cap.release()
        
        # VideoMAE ì…ë ¥ í˜•ì‹: (C, T, H, W)
        frames = torch.stack(frames).permute(1, 0, 2, 3) 
        return frames, label

# ==========================================
# ğŸ”¥ 3. VideoMAE í•™ìŠµ í•µì‹¬ í•¨ìˆ˜
# ==========================================
def train_videomae(dataset_name, epochs=5):
    # ì‹¤ì œ í´ë” êµ¬ì¡° ë§¤ì¹­ ìˆ˜ì •
    folder_map = {
        "pure": os.path.join("2_exp_train_pure", "train"),
        "mixed": "2_train_mixed",
        "worst": "2_train_worst"
    }
    target_folder = folder_map[dataset_name]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"\n==================================================")
    print(f"ğŸ”¥ [VideoMAE í•™ìŠµ] ë°ì´í„°ì…‹: {dataset_name.upper()}")
    print(f"==================================================")
    
    data_path = os.path.join(BASE_DIR, target_folder)
    dataset = VideoSequenceDataset(data_path, SEQUENCE_LENGTH, transform)
    if len(dataset) == 0: 
        print(f"âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return
    
    # RTX 4070 SUPER 12GB ê¸°ì¤€: ë°°ì¹˜ ì‚¬ì´ì¦ˆ 4ê°€ ê¶Œì¥ë˜ë‚˜ OOM ë°œìƒ ì‹œ 2ë¡œ ë‚®ì¶”ì„¸ìš”.
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0) 
    
    print("ğŸ“¥ VideoMAE ê³µì‹ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = VideoMAEForVideoClassification.from_pretrained(
        "MCG-NJU/videomae-base", 
        num_labels=2,
        ignore_mismatched_sizes=True
    )
    model = model.to(device)
    
    # Transformer ëª¨ë¸ì—ëŠ” AdamW ì˜µí‹°ë§ˆì´ì €ê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤.
    optimizer = optim.AdamW(model.parameters(), lr=2e-5)
    
    model.train()
    for epoch in range(epochs):
        loop = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for inputs, labels in loop:
            inputs, labels = inputs.to(device), labels.to(device)
            # VideoMAE ì…ë ¥ ê·œê²©: (B, T, C, H, W)
            inputs = inputs.permute(0, 2, 1, 3, 4) 
            
            optimizer.zero_grad()
            outputs = model(pixel_values=inputs, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            
            logits = outputs.logits
            _, predicted = torch.max(logits, 1)
            correct = (predicted == labels).sum().item()
            acc = 100 * correct / labels.size(0)

            loop.set_postfix(loss=loss.item(), acc=acc)
            
    save_name = f"model_temporal_videomae_{dataset_name}.pth"
    torch.save(model.state_dict(), save_name)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ ë° ì €ì¥: {save_name}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default="all")
    args = parser.parse_args()
    
    datasets_list = ["pure", "mixed", "worst"] if args.dataset == "all" else [args.dataset]
    for d in datasets_list:
        train_videomae(d, epochs=5)