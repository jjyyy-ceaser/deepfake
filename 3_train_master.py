import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
import os
import glob
import sys
import gc
import pandas as pd
import numpy as np
from tqdm import tqdm
from utils import DeepfakeDataset, get_model
from torchvision import transforms
from torch.cuda.amp import GradScaler, autocast # ìµœì‹  pytorchë¼ë©´ torch.amp ì‚¬ìš© ê¶Œì¥

# ==========================================
# âš™ï¸ ë³¸ í•™ìŠµ(Main Training) ì„¤ì •
# ==========================================
TARGET_DATASETS = ["dataset_B_mixed"] # í•„ìš”ì‹œ ["dataset_A_pure", "dataset_C_worst"] ì¶”ê°€ ê°€ëŠ¥
NUM_EPOCHS = 30       # ì¶©ë¶„í•œ í•™ìŠµì„ ìœ„í•´ 30ìœ¼ë¡œ ì„¤ì • (Early Stopping ìˆìŒ)
PATIENCE = 5          # 5ë²ˆ ì—°ì† ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
BATCH_SIZE_SPATIAL = 32
BATCH_SIZE_TEMPORAL = 16 # VRAM ì•ˆì „ê°’
NUM_WORKERS = 2       # ìœˆë„ìš° í™˜ê²½ ì¶©ëŒ ë°©ì§€
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ† Grid Searchë¡œ ì°¾ì€ ìµœì ì˜ LR (Learning Rates)
BEST_PARAMS = {
    "xception": 5e-5,   
    "convnext": 1e-4,   
    "swin": 5e-5,       
    "r3d": 1e-4,        # Grid Search ê²°ê³¼ ë°˜ì˜ (0.96)
    "r2plus1d": 5e-5    # Grid Search ê²°ê³¼ ë°˜ì˜ (0.93)
}

# í•™ìŠµí•  ëª¨ë¸ ëª©ë¡
MODELS_TO_TRAIN = ["r3d", "r2plus1d", "xception", "convnext", "swin"]

def get_transforms(model_name):
    """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì „ì²˜ë¦¬(Normalization) ë¶„ë¦¬"""
    if model_name in ["r3d", "r2plus1d", "videomae"]:
        # Temporal ëª¨ë¸ ì „ìš© (Kinetics-400 í†µê³„ê°’ ë“±)
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])
        ])
    else:
        # Spatial ëª¨ë¸ ì „ìš© (ImageNet í†µê³„ê°’)
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

def train_one_fold(fold_idx, train_files, train_labels, val_files, val_labels, model_name, dataset_name):
    print(f"\nğŸš€ [Fold {fold_idx+1}] Model: {model_name} | Dataset: {dataset_name}")
    
    # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    lr = BEST_PARAMS.get(model_name, 1e-4) # ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1e-4
    is_temporal = model_name in ["r3d", "r2plus1d", "videomae"]
    batch_size = BATCH_SIZE_TEMPORAL if is_temporal else BATCH_SIZE_SPATIAL
    tf = get_transforms(model_name)
    
    # ë°ì´í„°ì…‹ & ë¡œë”
    ds_tr = DeepfakeDataset(train_files, train_labels, 'temporal' if is_temporal else 'spatial', tf)
    ds_val = DeepfakeDataset(val_files, val_labels, 'temporal' if is_temporal else 'spatial', tf)
    
    l_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)
    l_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)
    
    # ëª¨ë¸ & ìµœì í™”
    model = get_model(model_name, DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss().to(DEVICE)
    scaler = GradScaler()
    
    # ì €ì¥ ê²½ë¡œ
    save_dir = os.path.join("checkpoints", dataset_name, model_name)
    os.makedirs(save_dir, exist_ok=True)
    best_path = os.path.join(save_dir, f"best_fold{fold_idx+1}.pth")
    
    best_auc = 0.0
    patience_counter = 0
    
    # === Epoch Loop ===
    for epoch in range(NUM_EPOCHS):
        model.train()
        train_loss = 0
        loop = tqdm(l_tr, desc=f"  Ep {epoch+1}/{NUM_EPOCHS}", leave=False, ncols=80)
        
        for x, y in loop:
            x, y = x.to(DEVICE), y.to(DEVICE, dtype=torch.long)
            optimizer.zero_grad()
            
            with autocast():
                # ì°¨ì› ë³´ì • (3D ëª¨ë¸ì€ permute í•„ìš”)
                out = model(x.permute(0, 2, 1, 3, 4)) if is_temporal else model(x)
                loss = criterion(out, y)
                
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            train_loss += loss.item()
            
        # ê²€ì¦ (Validation)
        model.eval()
        preds, trues = [], []
        with torch.no_grad():
            for vx, vy in l_val:
                vx = vx.to(DEVICE)
                vout = model(vx.permute(0, 2, 1, 3, 4)) if is_temporal else model(vx)
                preds.extend(torch.softmax(vout, 1)[:, 1].cpu().tolist())
                trues.extend(vy.tolist())
        
        try:
            val_auc = roc_auc_score(trues, preds)
        except:
            val_auc = 0.5
            
        print(f"    âœ… Ep {epoch+1} | Loss: {train_loss/len(l_tr):.4f} | Val AUC: {val_auc:.4f}")
        
        # Best Model ì €ì¥ & Early Stopping
        if val_auc > best_auc:
            best_auc = val_auc
            patience_counter = 0
            torch.save(model.state_dict(), best_path)
            print(f"      ğŸ’¾ Best Saved! ({best_auc:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print(f"      ğŸ›‘ Early Stopping (No improve for {PATIENCE} epochs)")
                break
                
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model, optimizer, scaler   
    torch.cuda.empty_cache()
    gc.collect()
    
    return best_auc

# === Main Execution ===
if __name__ == "__main__":
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì „ì¥ì¹˜
    import torch.multiprocessing as mp
    try: mp.set_start_method('spawn', force=True)
    except: pass

    results_log = []

    for dataset_name in TARGET_DATASETS:
        print(f"\n\n{'='*40}\nğŸ¯ Target Dataset: {dataset_name}\n{'='*40}")
        
        # ë°ì´í„° íŒŒì¼ ë¡œë“œ
        base_path = os.path.join("dataset", "final_datasets", dataset_name)
        real_files = glob.glob(os.path.join(base_path, "real", "*"))
        fake_files = glob.glob(os.path.join(base_path, "fake", "*"))
        
        if not real_files:
            print(f"âŒ ë°ì´í„° ì—†ìŒ: {base_path}")
            continue
            
        all_files = real_files + fake_files
        all_labels = [0]*len(real_files) + [1]*len(fake_files)
        
        # 5-Fold Setting
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for model_name in MODELS_TO_TRAIN:
            fold_scores = []
            
            for fold, (t_idx, v_idx) in enumerate(skf.split(all_files, all_labels)):
                # ê° í´ë“œë³„ í•™ìŠµ ì‹¤í–‰
                tr_files = [all_files[i] for i in t_idx]
                tr_labels = [all_labels[i] for i in t_idx]
                val_files = [all_files[i] for i in v_idx]
                val_labels = [all_labels[i] for i in v_idx]
                
                score = train_one_fold(fold, tr_files, tr_labels, val_files, val_labels, model_name, dataset_name)
                fold_scores.append(score)
            
            # ìµœì¢… ê²°ê³¼ ê¸°ë¡
            avg_score = np.mean(fold_scores)
            print(f"\nğŸ† {model_name} on {dataset_name} | Avg AUC: {avg_score:.4f} {fold_scores}")
            results_log.append({"Dataset": dataset_name, "Model": model_name, "Avg_AUC": avg_score, "Folds": fold_scores})
            
            # ëª¨ë¸ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

    # ìµœì¢… ë¦¬í¬íŠ¸ ì €ì¥
    pd.DataFrame(results_log).to_csv("final_training_results.csv", index=False)
    print("\nğŸ‰ ëª¨ë“  ë³¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'final_training_results.csv'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")