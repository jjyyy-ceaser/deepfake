import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import timm
import torchvision.models.video as video_models
import cv2
import os
import glob
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# --- [ì„¤ì •ê°’] ---
BATCH_SIZE = 4        
EPOCHS = 5            
LEARNING_RATE = 1e-4  
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DATA_DIR = "dataset"  

print(f"ğŸ”§ í•™ìŠµ ì¥ì¹˜ ì„¤ì •: {DEVICE}")

# ==========================================
# 1. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì§§ì€ ì˜ìƒ íŒ¨ë”© ê¸°ëŠ¥ í¬í•¨)
# ==========================================
class DeepfakeDataset(Dataset):
    def __init__(self, video_paths, labels, num_frames=16, transform=None):
        self.video_paths = video_paths
        self.labels = labels
        self.num_frames = num_frames 

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        path = self.video_paths[idx]
        label = self.labels[idx]
        
        cap = cv2.VideoCapture(path)
        frames = []
        while len(frames) < self.num_frames:
            ret, frame = cap.read()
            if not ret: break
            frame = cv2.resize(frame, (224, 224))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        cap.release()

        # ì˜ìƒì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê¹¨ì§„ ê²½ìš° (ê²€ì€ í™”ë©´ìœ¼ë¡œ ëŒ€ì²´)
        if not frames:
            frames = [np.zeros((224, 224, 3), dtype=np.uint8)] * self.num_frames
        
        # â­ í•µì‹¬: í”„ë ˆì„ì´ ëª¨ìë¼ë©´ ë§ˆì§€ë§‰ ì¥ë©´ì„ ë³µì‚¬í•´ì„œ ì±„ìš´ë‹¤ (Padding)
        while len(frames) < self.num_frames:
            frames.append(frames[-1])

        frames_np = np.array(frames, dtype=np.float32) / 255.0 
        video_tensor = torch.tensor(frames_np).permute(3, 0, 1, 2) 

        return video_tensor, label

# ==========================================
# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ==========================================
print("\nğŸ“‚ ë°ì´í„°ì…‹ ìŠ¤ìº” ì¤‘...")
real_videos = glob.glob(os.path.join(DATA_DIR, "real", "*.mp4"))
fake_videos = glob.glob(os.path.join(DATA_DIR, "fake", "*.mp4"))

# ë°ì´í„° í™•ì¸
if not real_videos and not fake_videos:
    print("âš ï¸ [ì˜¤ë¥˜] ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! dataset í´ë” ìœ„ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    import sys; sys.exit()

print(f"   - Real ì˜ìƒ: {len(real_videos)}ê°œ")
print(f"   - Fake ì˜ìƒ: {len(fake_videos)}ê°œ")

paths = real_videos + fake_videos
labels = [0] * len(real_videos) + [1] * len(fake_videos) 

# ë°ì´í„° ë¶„í• 
train_paths, test_paths, train_labels, test_labels = train_test_split(
    paths, labels, test_size=0.2, random_state=42, shuffle=True
)

train_dataset = DeepfakeDataset(train_paths, train_labels)
test_dataset = DeepfakeDataset(test_paths, test_labels)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ==========================================
# 3. ëª¨ë¸ ì •ì˜
# ==========================================
def get_xception():
    try:
        model = timm.create_model('xception', pretrained=True, num_classes=2)
    except:
        model = timm.create_model('legacy_xception', pretrained=True, num_classes=2)
    return model.to(DEVICE)

def get_r3d():
    model = video_models.r3d_18(pretrained=True)
    model.fc = nn.Linear(model.fc.in_features, 2)
    return model.to(DEVICE)

# ==========================================
# 4. í•™ìŠµ í•¨ìˆ˜
# ==========================================
def train_model(model, model_name):
    print(f"\nğŸš€ [{model_name}] í•™ìŠµ ì‹œì‘...")
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    for epoch in range(EPOCHS):
        model.train()
        correct = 0
        total = 0
        
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for videos, labels in loop:
            videos, labels = videos.to(DEVICE), labels.to(DEVICE)
            
            if model_name == "Xception":
                inputs = videos[:, :, 0, :, :] 
            else: 
                inputs = videos

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            loop.set_postfix(acc=100*correct/total)

    print(f"âœ… [{model_name}] í•™ìŠµ ì™„ë£Œ! ì •í™•ë„: {100*correct/total:.2f}%")
    return model

# ==========================================
# 5. ì‹¤í–‰
# ==========================================
if __name__ == "__main__":
    spatial_model = get_xception()
    spatial_model = train_model(spatial_model, "Xception")
    torch.save(spatial_model.state_dict(), "xception_result.pth")

    temporal_model = get_r3d()
    temporal_model = train_model(temporal_model, "R3D-18")
    torch.save(temporal_model.state_dict(), "r3d_result.pth")

    print("\nğŸ‰ ëª¨ë“  ì‹¤í—˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")