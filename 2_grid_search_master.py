import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
import os, glob, sys, gc
import pandas as pd
from tqdm import tqdm
from utils import DeepfakeDataset, get_model
from torchvision import transforms
from torch.cuda.amp import GradScaler, autocast

# ==========================================
# âš™ï¸ Final Grid Search ì„¤ì • (VideoMAE ì „ìš© ëª¨ë“œ)
# ==========================================
TARGET_DATASET = "dataset_B_mixed" 
LR_LIST = [1e-4, 5e-5, 1e-5]
NUM_WORKERS = 0 
DEVICE = torch.device("cuda")

# 1 Epochì€ ë„ˆë¬´ ì§§ìŒ. 3 Epochìœ¼ë¡œ ëŠ˜ë ¤ ì‹ ë¢°ë„ í™•ë³´ (ê¸°ì¡´ê³¼ ë™ì¼)
TEST_EPOCHS = 3 

# [ìˆ˜ì • í¬ì¸íŠ¸ 1] VideoMAE ëª¨ë¸ë§Œ íƒìƒ‰í•˜ë„ë¡ ê·¸ë£¹ ì¬ì„¤ì •
SPATIAL_MODELS = []
TEMPORAL_MODELS = ["videomae"]

def get_group_config(group_name):
    if group_name == 'TEMPORAL':
        tf = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])
        ])
        return tf, 16
    else:
        tf = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        return tf, 32

def run_grid_search_group(models, group_name):
    if not models:
        return []

    print(f"\nğŸš€ Starting Grid Search Group: {group_name} (Epochs: {TEST_EPOCHS})")
    tf, bs = get_group_config(group_name)
    
    base = os.path.join("dataset", "final_datasets", TARGET_DATASET)
    real = glob.glob(os.path.join(base, "real", "*"))
    fake = glob.glob(os.path.join(base, "fake", "*"))
    
    if len(real) == 0 or len(fake) == 0:
        print(f"âŒ Error: ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {base}")
        return []

    files, labels = real + fake, [0]*len(real) + [1]*len(fake)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    t_idx, v_idx = next(skf.split(files, labels)) 
    
    group_results = []
    
    for model_name in models:
        print(f"  ğŸ”¹ Model: {model_name} (Batch: {bs})")
        is_temp = group_name == 'TEMPORAL'
        
        ds_tr = DeepfakeDataset([files[i] for i in t_idx], [labels[i] for i in t_idx], 'temporal' if is_temp else 'spatial', tf)
        ds_val = DeepfakeDataset([files[i] for i in v_idx], [labels[i] for i in v_idx], 'temporal' if is_temp else 'spatial', tf)
        
        for lr in LR_LIST:
            try:
                l_tr = DataLoader(ds_tr, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS)
                l_val = DataLoader(ds_val, batch_size=bs, shuffle=False, num_workers=NUM_WORKERS)
                
                model = get_model(model_name, DEVICE)
                optimizer = optim.AdamW(model.parameters(), lr=lr)
                criterion = nn.CrossEntropyLoss().to(DEVICE)
                scaler = GradScaler()
                
                best_epoch_auc = 0.0
                
                # ì—í¬í¬ ë°˜ë³µ ë£¨í”„ (ê¸°ì¡´ê³¼ ë™ì¼)
                for ep in range(TEST_EPOCHS):
                    model.train()
                    for x, y in tqdm(l_tr, desc=f"    LR={lr} | Ep={ep+1}", leave=False, ncols=80):
                        x, y = x.to(DEVICE), y.to(DEVICE, dtype=torch.long)
                        optimizer.zero_grad()
                        with autocast():
                            out = model(x.permute(0, 2, 1, 3, 4)) if is_temp else model(x)
                            loss = criterion(out, y)
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()

                    # ê²€ì¦ (ë§¤ ì—í¬í¬ë§ˆë‹¤ í™•ì¸)
                    model.eval()
                    preds, trues = [], []
                    with torch.no_grad():
                        for vx, vy in l_val:
                            vx = vx.to(DEVICE)
                            vout = model(vx.permute(0, 2, 1, 3, 4)) if is_temp else model(vx)
                            preds.extend(torch.softmax(vout, 1)[:, 1].cpu().tolist())
                            trues.extend(vy.tolist())
                    
                    epoch_auc = roc_auc_score(trues, preds) if len(set(trues)) > 1 else 0.5
                    
                    # 3ë²ˆ ì¤‘ ê°€ì¥ ì˜ ë‚˜ì˜¨ ì ìˆ˜ë¥¼ ê¸°ë¡
                    if epoch_auc > best_epoch_auc:
                        best_epoch_auc = epoch_auc
                
                print(f"    âœ… LR={lr} | Best AUC (over {TEST_EPOCHS} eps)={best_epoch_auc:.4f}")
                group_results.append({"Model": model_name, "LR": lr, "AUC": best_epoch_auc})
                
                del model, optimizer, scaler
                torch.cuda.empty_cache()
                gc.collect()

            except Exception as e:
                print(f"    âŒ Error at LR={lr}: {e}")
                torch.cuda.empty_cache()
                gc.collect()
                continue
                
    return group_results

if __name__ == "__main__":
    import torch.multiprocessing as mp
    try: mp.set_start_method('spawn', force=True)
    except: pass
    
    final_res = []
    
    # ê³µê°„ì  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ëŠ” ë¹„ì–´ìˆìœ¼ë¯€ë¡œ ë°”ë¡œ í†µê³¼ë©ë‹ˆë‹¤.
    final_res.extend(run_grid_search_group(SPATIAL_MODELS, "SPATIAL"))
    
    torch.cuda.empty_cache()
    gc.collect()
    
    # ì‹œê°„ì  ëª¨ë¸(VideoMAE) íƒìƒ‰ ìˆ˜í–‰
    final_res.extend(run_grid_search_group(TEMPORAL_MODELS, "TEMPORAL"))
    
    # [ìˆ˜ì • í¬ì¸íŠ¸ 2] ê¸°ì¡´ ê²°ê³¼ë¥¼ ë®ì–´ì“°ì§€ ì•Šê³  ì•ˆì „í•˜ê²Œ 'ì´ì–´ ì“°ê¸°(Append)'
    if final_res:
        csv_file = "grid_search_master_results.csv"
        new_df = pd.DataFrame(final_res)
        
        if os.path.exists(csv_file):
            # 1. ê¸°ì¡´ CSV íŒŒì¼ ì½ì–´ì˜¤ê¸°
            existing_df = pd.read_csv(csv_file)
            # 2. í˜¹ì‹œ ì´ì „ì— ì‹¤í–‰í•˜ë‹¤ ì¤‘ë‹¨ëœ videomae ê¸°ë¡ì´ ìˆë‹¤ë©´ ì‚­ì œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            existing_df = existing_df[existing_df['Model'] != 'videomae']
            # 3. ê¸°ì¡´ ê¸°ë¡ ë°‘ì— ìƒˆë¡œìš´ videomae ê¸°ë¡ ë¶™ì´ê¸°
            updated_df = pd.concat([existing_df, new_df], ignore_index=True)
            updated_df.to_csv(csv_file, index=False)
            print(f"\nğŸ’¾ ì„±ê³µ: ê¸°ì¡´ '{csv_file}' íŒŒì¼ì— VideoMAEì˜ LRê³¼ AUC ê²°ê³¼ê°€ ì•ˆì „í•˜ê²Œ ì´ì–´ ì“°ê¸° ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ìƒˆë¡œ ìƒì„±
            new_df.to_csv(csv_file, index=False)
            print(f"\nğŸ’¾ ìƒˆë¡œìš´ '{csv_file}' íŒŒì¼ì´ ìƒì„±ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ê¸°ë¡í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")