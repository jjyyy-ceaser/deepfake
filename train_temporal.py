import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
import cv2
import os
import numpy as np
from tqdm import tqdm
import argparse

# ==========================================
# âš™ï¸ ì„¤ì •
# ==========================================
BASE_DIR = "C:/Users/leejy/Desktop/test_experiment/dataset"
SEQUENCE_LENGTH = 16
IMG_SIZE = 112  # R3D, R(2+1)D ëª¨ë¸ì˜ í‘œì¤€ ì…ë ¥ ì‚¬ì´ì¦ˆ

# R3D ë° R(2+1)Dë¥¼ ìœ„í•œ ì •ê·œí™” ê°’ (Kinetics-400 ë°ì´í„°ì…‹ ê¸°ì¤€)
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])
])

# ==========================================
# ğŸ“‚ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ==========================================
class VideoSequenceDataset(Dataset):
    def __init__(self, data_dir, sequence_length=16, transform=None):
        self.data_dir = data_dir
        self.seq_len = sequence_length
        self.transform = transform
        self.samples = []
        
        real_dir = os.path.join(data_dir, "real")
        fake_dir = os.path.join(data_dir, "fake")
        
        # mp4 íŒŒì¼ë§Œ ê³¨ë¼ë‚´ì–´ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        if os.path.exists(real_dir):
            for f in os.listdir(real_dir):
                if f.lower().endswith('.mp4'): self.samples.append((os.path.join(real_dir, f), 0))
        if os.path.exists(fake_dir):
            for f in os.listdir(fake_dir):
                if f.lower().endswith('.mp4'): self.samples.append((os.path.join(fake_dir, f), 1))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        video_path, label = self.samples[idx]
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # ëœë¤í•œ ì‹œì ì—ì„œ ì‹œí€€ìŠ¤ ì¶”ì¶œ
        start_frame = 0
        if total_frames > self.seq_len:
            start_frame = np.random.randint(0, total_frames - self.seq_len)
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        frames = []
        for _ in range(self.seq_len):
            ret, frame = cap.read()
            if not ret:
                # í”„ë ˆì„ ë¶€ì¡± ì‹œ ê²€ì€ í™”ë©´ìœ¼ë¡œ íŒ¨ë”©
                frame = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)
            else:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if self.transform:
                frame = self.transform(frame)
            frames.append(frame)
        cap.release()
        
        # VideoMAEì™€ ë‹¬ë¦¬ (C, T, H, W) í˜•ì‹ì´ í•„ìš”í•¨
        frames = torch.stack(frames).permute(1, 0, 2, 3) 
        return frames, label

# ==========================================
# ğŸ—ï¸ ëª¨ë¸ ë¹Œë“œ í•¨ìˆ˜
# ==========================================
def get_model(model_name, device):
    print(f"ğŸ—ï¸ ëª¨ë¸ ë¹Œë“œ ì¤‘: {model_name.upper()}...")
    if model_name == "r3d":
        model = models.video.r3d_18(weights=models.video.R3D_18_Weights.KINETICS400_V1)
    elif model_name == "r2plus1d":
        model = models.video.r2plus1d_18(weights=models.video.R2Plus1D_18_Weights.KINETICS400_V1)
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.")
        
    model.fc = nn.Linear(model.fc.in_features, 2)
    return model.to(device)

# ==========================================
# ğŸ”¥ í•™ìŠµ í•µì‹¬ í•¨ìˆ˜
# ==========================================
def train_model(model_type, dataset_name, epochs=5):
    # í´ë” êµ¬ì¡° ë§¤ì¹­ ìˆ˜ì • (2ë²ˆ, 2_train_mixed, 2_train_worst ë°˜ì˜)
    folder_map = {
        "pure": os.path.join("2_exp_train_pure", "train"),
        "mixed": "2_train_mixed",
        "worst": "2_train_worst"
    }
    target_folder = folder_map[dataset_name]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"\n==================================================")
    print(f"ğŸ”¥ [Temporal í•™ìŠµ] ëª¨ë¸: {model_type.upper()} | ë°ì´í„°: {dataset_name.upper()}")
    print(f"==================================================")
    
    data_path = os.path.join(BASE_DIR, target_folder)
    dataset = VideoSequenceDataset(data_path, SEQUENCE_LENGTH, transform)
    if len(dataset) == 0:
        print(f"âŒ ë°ì´í„° ì—†ìŒ: {data_path}")
        return
    
    # RTX 4070 SUPER 12GB ê¸°ì¤€ìœ¼ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 8ì´ ì•ˆì •ì ì…ë‹ˆë‹¤.
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)
    model = get_model(model_type, device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)
    
    model.train()
    for epoch in range(epochs):
        loop = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for inputs, labels in loop:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            loop.set_postfix(loss=loss.item())
            
    save_name = f"model_temporal_{model_type}_{dataset_name}.pth"
    torch.save(model.state_dict(), save_name)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_name}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="all", help="r3d / r2plus1d / all")
    parser.add_argument("--dataset", type=str, default="all", help="pure / mixed / worst / all")
    args = parser.parse_args()
    
    target_models = ["r3d", "r2plus1d"] if args.model == "all" else [args.model]
    target_datasets = ["pure", "mixed", "worst"] if args.dataset == "all" else [args.dataset]
    
    for m in target_models:
        for d in target_datasets:
            train_model(m, d, epochs=5)