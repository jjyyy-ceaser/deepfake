import torch
import pandas as pd
import glob
import os
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
from utils import DeepfakeDataset, get_model
from torch.utils.data import DataLoader
from torchvision import transforms

# === ì„¤ì • ===
DOMAINS = ["svd", "pika", "runway", "ffpp"]
CASES = ["case1_original", "case2_lowres", "case3_compress", "case4_mixed"]
TRAIN_SETS = ["dataset_A_pure", "dataset_C_worst", "dataset_B_mixed"]
MODELS = ["xception", "convnext", "swin", "r3d", "r2plus1d", "videomae_v2"]
DEVICE = torch.device("cuda")

def run_evaluation():
    results = []
    tf = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224,224)), transforms.ToTensor()])
    
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ëª¨ë“  ë„ë©”ì¸, ëª¨ë“  ì¼€ì´ìŠ¤ ë¯¸ë¦¬ ì¤€ë¹„)
    test_loaders = {}
    print("ğŸ“‚ Loading Test Data...")
    for dom in DOMAINS:
        for case in CASES:
            # Fake/Real êµ¬ë¶„ ì—†ì´ í•´ë‹¹ í´ë” ë‹¤ ì½ì–´ì„œ ë¼ë²¨ë§ (FF++ì€ Real/Fake ì„ì—¬ìˆìŒ ì£¼ì˜)
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ: FF++=Real(0), ë‚˜ë¨¸ì§€=Fake(1)ë¡œ ê°€ì • (ì‹¤ì œ íŒŒì¼ëª…/í´ë”êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì • í•„ìš”)
            label_val = 0 if dom == 'ffpp' else 1 
            path = os.path.join("dataset", "processed_cases", "test", case, dom)
            files = glob.glob(os.path.join(path, "*"))
            
            # ëª¨ë¸ íƒ€ì…(Spatial/Temporal)ì— ë”°ë¼ ë¡œë”ê°€ ë‹¬ë¼ì ¸ì•¼ í•˜ë¯€ë¡œ íŒŒì¼ë¦¬ìŠ¤íŠ¸ë§Œ ì €ì¥
            test_loaders[f"{dom}_{case}"] = (files, [label_val]*len(files))

    # 2. í‰ê°€ ë£¨í”„
    for train_set in TRAIN_SETS:
        for model_name in MODELS:
            print(f"ğŸ“Š Eval: {train_set} | {model_name}")
            model_type = 'temporal' if any(x in model_name for x in ['r3d', 'r2', 'mae']) else 'spatial'
            is_mae = 'mae' in model_name
            
            # 5-Fold ëª¨ë¸ ë¡œë“œ ë° ì•™ìƒë¸” ì¤€ë¹„
            models_fold = []
            for fold in range(5):
                pth = os.path.join("checkpoints", train_set, model_name, f"fold{fold}_best.pth")
                if not os.path.exists(pth): continue
                m = get_model(model_name, DEVICE)
                m.load_state_dict(torch.load(pth))
                m.eval()
                models_fold.append(m)

            # 16ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
            for dom in DOMAINS:
                for case in CASES:
                    files, labels = test_loaders[f"{dom}_{case}"]
                    if len(files) == 0: continue
                    
                    ds = DeepfakeDataset(files, labels, model_type, tf)
                    loader = DataLoader(ds, batch_size=8, shuffle=False)
                    
                    all_preds = [] # (N_samples, 5_folds)
                    all_labels = []
                    
                    # 5ê°œ ëª¨ë¸ ì˜ˆì¸¡ í‰ê· 
                    with torch.no_grad():
                        for x, y in loader:
                            x = x.to(DEVICE)
                            fold_outputs = []
                            for m in models_fold:
                                if is_mae: out = m(pixel_values=x.permute(0,2,1,3,4)).logits
                                else: out = m(x)
                                fold_outputs.append(torch.softmax(out, 1)[:, 1].cpu().numpy())
                            
                            avg_pred = np.mean(fold_outputs, axis=0)
                            all_preds.extend(avg_pred)
                            all_labels.extend(y.numpy())
                    
                    # ì§€í‘œ ê³„ì‚°
                    preds_binary = [1 if p > 0.5 else 0 for p in all_preds]
                    res = {
                        "Train_Set": train_set,
                        "Model": model_name,
                        "Test_Domain": dom,
                        "Test_Case": case,
                        "AUC": roc_auc_score(all_labels, all_preds) if len(set(all_labels))>1 else 0,
                        "ACC": accuracy_score(all_labels, preds_binary),
                        "F1": f1_score(all_labels, preds_binary, zero_division=0),
                        "Pre": precision_score(all_labels, preds_binary, zero_division=0),
                        "Rec": recall_score(all_labels, preds_binary, zero_division=0)
                    }
                    results.append(res)

    # 3. ì €ì¥
    df = pd.DataFrame(results)
    df.to_excel("Final_Robustness_Report.xlsx", index=False)
    print("ğŸ‰ ì‹¤í—˜ ì¢…ë£Œ! ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ.")

if __name__ == "__main__":
    run_evaluation()